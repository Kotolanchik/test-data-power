# Ozon Data Pipeline
Данный проект представляет собой ETL-пайплайн для обработки данных заказов Ozon. Он включает загрузку данных из JSON-файлов в ODS слой, проверку данных, обработку ошибок и миграцию в DDS слой с полным логированием всех этапов процесса.

# Запуск
## Создайте сеть для докера: 
```docker network create ozon-net```

## Запустите команду для формирования контейнеров: 
```docker-compose up```

## Для входа через dbeaver используйте следующие креды:
1. host: localhost
2. port: 5433
3. pass: airflow
4. schema: public
5. db: airflow

## Организация пайплайна
Процесс одного запуска завернут в батч. Батч состоит из сессий. Сессии состоят из джобов. Например, в нашем случае батч состоит из двух сессий: загрузка в одс и загрузка в ддс. Внутренности завернуты уже в джобы.

Процесс проверки дубликатов реализован в методе self.db.fetch_existing_ods_orders (data_processor.py)

Логика загрузки: сначала грузим в слой с сырыми данными, далее в якобы ядро. (схем отдельных нет, все в пабе)

## Вход в юзер интерфейс:
1. Airflow UI: http://localhost:8080
2. Логин: airflow
3. Пароль: airflow
